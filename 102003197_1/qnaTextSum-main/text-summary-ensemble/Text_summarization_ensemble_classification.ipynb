{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "from nltk.tag import pos_tag # for proper noun\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import math\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "ps = PorterStemmer()\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ......................part 1 (cue phrases).................\n",
    "def cue_phrases():\n",
    "    QPhrases=[\"incidentally\", \"example\", \"anyway\", \"furthermore\",\"according\",\n",
    "            \"first\", \"second\", \"then\", \"now\", \"thus\", \"moreover\", \"therefore\", \"hence\", \"lastly\", \"finally\", \"summary\"]\n",
    "\n",
    "    cue_phrases={}\n",
    "    for sentence in sent_tokens:\n",
    "        cue_phrases[sentence] = 0\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        for word in word_tokens:\n",
    "            if word.lower() in QPhrases:\n",
    "                cue_phrases[sentence] += 1\n",
    "    maximum_frequency = max(cue_phrases.values())\n",
    "    for k in cue_phrases.keys():\n",
    "        try:\n",
    "            cue_phrases[k] = cue_phrases[k] / maximum_frequency\n",
    "            cue_phrases[k]=round(cue_phrases[k],3)\n",
    "        except ZeroDivisionError:\n",
    "            x=0\n",
    "    print(cue_phrases.values())\n",
    "    return cue_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .......................part2 (numerical data)...................\n",
    "def numeric_data():\n",
    "    numeric_data={}\n",
    "    for sentence in sent_tokens:\n",
    "        numeric_data[sentence] = 0\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        for k in word_tokens:\n",
    "            if k.isdigit():\n",
    "                numeric_data[sentence] += 1\n",
    "    maximum_frequency = max(numeric_data.values())\n",
    "    for k in numeric_data.keys():\n",
    "        try:\n",
    "            numeric_data[k] = (numeric_data[k]/maximum_frequency)\n",
    "            numeric_data[k] = round(numeric_data[k], 3)\n",
    "        except ZeroDivisionError:\n",
    "            x=0\n",
    "    print(numeric_data.values())\n",
    "    return numeric_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#....................part3(sentence length)........................\n",
    "def sent_len_score():\n",
    "    sent_len_score={}\n",
    "    for sentence in sent_tokens:\n",
    "        sent_len_score[sentence] = 0\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        length=len(word_tokens)\n",
    "        if length in range(0,10):\n",
    "            sent_len_score[sentence]=1-0.05*(10-length)\n",
    "        elif len(word_tokens) in range(10,20):\n",
    "            sent_len_score[sentence]=1\n",
    "        else:\n",
    "            sent_len_score[sentence]=1-(0.05)*(length-20)\n",
    "    for k in sent_len_score.keys():\n",
    "        sent_len_score[k]=round(sent_len_score[k],4)\n",
    "    print(sent_len_score.values())\n",
    "    return sent_len_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#....................part4(sentence position)........................\n",
    "def sentence_position():\n",
    "    sentence_position={}\n",
    "    d=1\n",
    "    no_of_sent=len(sent_tokens)\n",
    "    for i in range(no_of_sent):\n",
    "        a=1/d\n",
    "        b=1/(no_of_sent-d+1)\n",
    "        sentence_position[sent_tokens[d-1]]=max(a,b)\n",
    "        d=d+1\n",
    "    for k in sentence_position.keys():\n",
    "        sentence_position[k]=round(sentence_position[k],3)\n",
    "    print(sentence_position.values())\n",
    "    return sentence_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a frequency table to compute the frequency of each word.\n",
    "def word_frequency():\n",
    "    freqTable = {}\n",
    "    for word in word_tokens_refined:    \n",
    "        if word in freqTable:         \n",
    "            freqTable[word] += 1    \n",
    "        else:         \n",
    "            freqTable[word] = 1\n",
    "    for k in freqTable.keys():\n",
    "        freqTable[k]= math.log10(1+freqTable[k])\n",
    "#Compute word frequnecy score of each sentence\n",
    "    word_frequency={}\n",
    "    for sentence in sent_tokens:\n",
    "        word_frequency[sentence]=0\n",
    "        e=nltk.word_tokenize(sentence)\n",
    "        f=[]\n",
    "        for word in e:\n",
    "            f.append(ps.stem(word))\n",
    "        for word,freq in freqTable.items():\n",
    "            if word in f:\n",
    "                word_frequency[sentence]+=freq\n",
    "    maximum=max(word_frequency.values())\n",
    "    for key in word_frequency.keys():\n",
    "        try:\n",
    "            word_frequency[key]=word_frequency[key]/maximum\n",
    "            word_frequency[key]=round(word_frequency[key],3)\n",
    "        except ZeroDivisionError:\n",
    "            x=0\n",
    "    print(word_frequency.values())\n",
    "    return word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#........................part 6 (upper cases).................................\n",
    "def upper_case():\n",
    "    upper_case={}\n",
    "    for sentence in sent_tokens:\n",
    "        upper_case[sentence] = 0\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        for k in word_tokens:\n",
    "            if k.isupper():\n",
    "                upper_case[sentence] += 1\n",
    "    maximum_frequency = max(upper_case.values())\n",
    "    for k in upper_case.keys():\n",
    "        try:\n",
    "            upper_case[k] = (upper_case[k]/maximum_frequency)\n",
    "            upper_case[k] = round(upper_case[k], 3)\n",
    "        except ZeroDivisionError:\n",
    "            x=0\n",
    "    print(upper_case.values())\n",
    "    return upper_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#......................... part7 (number of proper noun)...................\n",
    "def proper_noun():\n",
    "    proper_noun={}\n",
    "    for sentence in sent_tokens:\n",
    "        tagged_sent = pos_tag(sentence.split())\n",
    "        propernouns = [word for word, pos in tagged_sent if pos == 'NNP']\n",
    "        proper_noun[sentence]=len(propernouns)\n",
    "    maximum_frequency = max(proper_noun.values())\n",
    "    for k in proper_noun.keys():\n",
    "        try:\n",
    "            proper_noun[k] = (proper_noun[k]/maximum_frequency)\n",
    "            proper_noun[k] = round(proper_noun[k], 3)\n",
    "        except ZeroDivisionError:\n",
    "            x=0\n",
    "    print(proper_noun.values())\n",
    "    return proper_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#................................. part 8 (word matches with heading) ...................\n",
    "def head_match():\n",
    "    head_match={}\n",
    "    heading=sent_tokens[0]\n",
    "    for sentence in sent_tokens:\n",
    "        head_match[sentence]=0\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        for k in word_tokens:\n",
    "            if k not in stopWords:\n",
    "                k = ps.stem(k)\n",
    "                if k in ps.stem(heading):\n",
    "                    head_match[sentence] += 1\n",
    "    maximum_frequency = max(head_match.values())\n",
    "    for k in head_match.keys():\n",
    "        try:\n",
    "            head_match[k] = (head_match[k]/maximum_frequency)\n",
    "            head_match[k] = round(head_match[k], 3)\n",
    "        except ZeroDivisionError:\n",
    "            x=0\n",
    "    print(head_match.values())\n",
    "    return head_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def create_frequency_matrix(sentences):\n",
    "    frequency_matrix = {}\n",
    "\n",
    "    for sent in sentences:\n",
    "        freq_table = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = ps.stem(word)\n",
    "            if word in stopWords:\n",
    "                pass\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        frequency_matrix[sent] = freq_table\n",
    "\n",
    "    return frequency_matrix\n",
    "\n",
    "\n",
    "def create_tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count / count_words_in_sentence\n",
    "\n",
    "        tf_matrix[sent] = tf_table\n",
    "\n",
    "    return tf_matrix\n",
    "\n",
    "\n",
    "def create_documents_per_words(freq_matrix):\n",
    "    word_per_doc_table = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "          # occureance of word in sentences\n",
    "            if word in word_per_doc_table:\n",
    "                word_per_doc_table[word] += 1\n",
    "            else:\n",
    "                word_per_doc_table[word] = 1\n",
    "\n",
    "    return word_per_doc_table\n",
    "\n",
    "\n",
    "def create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
    "  # total_documents=total_sentences\n",
    "  # inverse document frequency\n",
    "    idf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "\n",
    "        for word in f_table.keys():\n",
    "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix\n",
    "\n",
    "\n",
    "def create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "\n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
    "                                                    f_table2.items()):  \n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "\n",
    "        tf_idf_matrix[sent1] = tf_idf_table\n",
    "\n",
    "    return tf_idf_matrix\n",
    "\n",
    "\n",
    "def score_sent(tf_idf_matrix):\n",
    "\n",
    "    sentenceValue = {}\n",
    "\n",
    "    for sent, f_table in tf_idf_matrix.items():\n",
    "        total_score_per_sentence = 0\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, score in f_table.items():\n",
    "            total_score_per_sentence += 0.5*score\n",
    "\n",
    "        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n",
    "\n",
    "    return sentenceValue\n",
    "\n",
    "\n",
    "def tfidf():\n",
    "    sentences = sent_tokens\n",
    "    total_documents = len(sentences)\n",
    "\n",
    "    freq_matrix = create_frequency_matrix(sentences)\n",
    "    #print(freq_matrix)\n",
    "\n",
    "    tf_matrix = create_tf_matrix(freq_matrix)\n",
    "    #print(tf_matrix)\n",
    "\n",
    "    count_doc_per_words = create_documents_per_words(freq_matrix)\n",
    "    #print(count_doc_per_words)\n",
    "\n",
    "    idf_matrix = create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
    "    \n",
    "    tf_idf_matrix = create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "\n",
    "    tfidf_score = score_sent(tf_idf_matrix)\n",
    "\n",
    "    print(tfidf_score.values())\n",
    "    return tfidf_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [a, b, c, d, upper, f, g, h, tfidf, key, label]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame(columns=['a','b','c','d','upper','f','g','h','tfidf','key','label'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0])\n",
      "dict_values([0.8, -2.25, 1, -0.1, 1.0, 1, 0.6, 0.35, 1, 0.85])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0])\n",
      "dict_values([0.87, 1.0, 0.304, 0.478, 0.13, 0.174, 0.13, 0.13, 0.261, 0.217])\n",
      "dict_values([0.484, 1.0, 0.189, 0.755, 0.238, 0.158, 0.305, 0.191, 0.212, 0.263])\n",
      "dict_values([0.364, 1.0, 0.0, 0.909, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
      "dict_values([0.01167164173408561, 0.005962954972471628, 0.01805993892231603, 0.011462550531802224, 0.01971493418932144, 0.02160297186066368, 0.013235712443217544, 0.01305082709345961, 0.01767904411100963, 0.015927200985254637])\n",
      "dict_values([0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n",
      "dict_values([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0])\n",
      "dict_values([1, 0.6, 1, 0.8, 0.8, -0.05, 1, 1, 1.0, 0.45, -0.35, -0.8, 1, 0.4, 0.15, 0.5])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.125, 0.125, 0.143, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([0.333, 0.667, 0.333, 0.667, 0.0, 0.0, 0.667, 0.0, 0.333, 0.333, 0.0, 0.333, 0.0, 0.0, 1.0, 0.333])\n",
      "dict_values([1.0, 0.455, 0.182, 0.182, 0.182, 0.273, 0.182, 0.182, 0.273, 0.091, 0.091, 0.364, 0.091, 0.182, 0.364, 0.364])\n",
      "dict_values([0.497, 0.904, 0.479, 0.545, 0.471, 0.545, 0.504, 0.516, 0.549, 0.569, 0.762, 0.894, 0.427, 0.667, 1.0, 0.53])\n",
      "dict_values([0.5, 0.625, 0.0, 0.125, 0.0, 0.0, 0.625, 0.125, 0.125, 0.125, 0.875, 1.0, 0.0, 0.25, 0.875, 0.125])\n",
      "dict_values([0.029855578474373187, 0.014726100063967172, 0.03022783531342829, 0.025203106465573832, 0.019573838685302703, 0.01423640082134037, 0.027734561204168038, 0.032933671270110966, 0.020310122129825237, 0.01865282259808212, 0.013112206536110366, 0.011407809156560717, 0.029358206959009734, 0.013608008354468045, 0.013312622482314731, 0.014587493578165953])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
      "dict_values([1, 0.6, -0.65, 0.5, 0.35, 0.9, 0.15, 0.5, 0.1, 0.9, 0.35, 0.95, -0.25, 0.95, 0.9, 1.0])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.125, 0.125, 0.143, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([0.25, 0.0, 0.75, 0.75, 0.5, 0.25, 1.0, 0.75, 0.0, 0.0, 0.5, 0.5, 1.0, 0.25, 0.5, 0.25])\n",
      "dict_values([1.0, 0.286, 0.429, 0.714, 0.714, 0.571, 0.857, 0.143, 0.286, 0.143, 0.143, 0.286, 0.857, 0.286, 0.429, 0.429])\n",
      "dict_values([0.427, 0.688, 1.0, 0.674, 0.815, 0.532, 0.833, 0.555, 0.588, 0.342, 0.761, 0.383, 0.982, 0.356, 0.474, 0.642])\n",
      "dict_values([0.2, 0.3, 0.4, 0.9, 0.7, 0.1, 0.8, 0.6, 0.0, 0.0, 0.6, 0.3, 1.0, 0.0, 0.1, 0.2])\n",
      "dict_values([0.022516607602070214, 0.017861269008703436, 0.012806902128398294, 0.01666892468334528, 0.015532933376483515, 0.01777251150083099, 0.0145163577354011, 0.016265558404984722, 0.015759395708810447, 0.04238577614439037, 0.012371166216401367, 0.034674474522968395, 0.010766060954321118, 0.018090224406819965, 0.019858771173214475, 0.018759724857303726])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5])\n",
      "dict_values([1, 0.15, 1, 0.7, -0.5, 0.55, 1, 0.75, -1.4, 1, 0.85, 1, 1.0, 0.5, -0.45, 1, 0.55])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.125, 0.111, 0.1, 0.111, 0.125, 0.143, 0.167, 0.5, 1.0, 0.333])\n",
      "dict_values([0.333, 0.333, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.333, 0.0, 1.0, 0.0, 0.0])\n",
      "dict_values([1.0, 0.375, 0.125, 0.125, 0.5, 0.5, 0.125, 0.125, 0.625, 0.125, 0.375, 0.25, 0.25, 0.375, 0.625, 0.125, 0.375])\n",
      "dict_values([0.432, 0.648, 0.213, 0.581, 0.786, 0.633, 0.276, 0.466, 1.0, 0.213, 0.662, 0.405, 0.384, 0.689, 0.823, 0.346, 0.689])\n",
      "dict_values([0.214, 0.214, 0.143, 0.143, 0.286, 0.357, 0.143, 0.0, 0.857, 0.0, 0.214, 0.0, 0.143, 0.214, 1.0, 0.0, 0.214])\n",
      "dict_values([0.030670271454149956, 0.013222516813772657, 0.022874741408316204, 0.021857687873067244, 0.017129191030705676, 0.016062553846077587, 0.03314882752606808, 0.020059277821960965, 0.008667958492538805, 0.041151600828326876, 0.018243609196118116, 0.03062458475341806, 0.019887661125386747, 0.013395301277924567, 0.013589888593880285, 0.027307362220870487, 0.013074550123384201])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([1, 0.7, 0.45, 1.0, 1, 0.55, 0.85, -0.85, 0.45, 0.3, 1.0, 0.9, 0.9, 0.3])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.143, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([0.0, 1.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 1.0, 0.0, 0.0, 0.5])\n",
      "dict_values([1.0, 0.273, 0.273, 0.273, 0.091, 0.273, 0.273, 0.545, 0.182, 0.182, 0.364, 0.091, 0.091, 0.273])\n",
      "dict_values([0.448, 0.77, 0.748, 0.421, 0.476, 0.515, 0.596, 1.0, 0.453, 0.558, 0.457, 0.209, 0.411, 0.644])\n",
      "dict_values([0.6, 1.0, 0.4, 0.2, 0.2, 0.6, 0.0, 0.4, 0.0, 0.2, 0.8, 0.0, 0.0, 0.2])\n",
      "dict_values([0.033832760630698504, 0.013510405758894471, 0.012542742518646628, 0.017784581286638988, 0.02806327379736073, 0.016571835411736117, 0.016469400861922927, 0.010564425693658524, 0.013359495559211057, 0.014340244294476815, 0.02044651680489114, 0.042436184708759116, 0.018670230461713448, 0.014446963115974152])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([1, 0.6, 0.0, 1, 0.2, 0.85, -0.2, 0.45, 0.95, 0.9, 1, 1, 0.75, 0.9, 1.0, 1, 0.15, 0.45, 1])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.125, 0.111, 0.1, 0.111, 0.125, 0.143, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([0.333, 1.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.333, 0.0, 0.0])\n",
      "dict_values([1.0, 0.182, 0.364, 0.182, 0.182, 0.273, 0.455, 0.273, 0.273, 0.182, 0.091, 0.091, 0.091, 0.091, 0.273, 0.091, 0.182, 0.273, 0.091])\n",
      "dict_values([0.608, 0.414, 0.928, 0.567, 0.876, 0.621, 1.0, 0.747, 0.457, 0.882, 0.519, 0.327, 0.213, 0.36, 0.482, 0.371, 0.826, 0.462, 0.389])\n",
      "dict_values([0.5, 0.167, 0.0, 0.0, 0.833, 0.0, 0.0, 0.0, 0.0, 0.333, 0.167, 0.0, 0.333, 0.0, 0.0, 0.167, 1.0, 0.0, 0.167])\n",
      "dict_values([0.02479721601506356, 0.017993012439559142, 0.01436543813402057, 0.023273548718639295, 0.013896437544845153, 0.019926061842707305, 0.014867031525483574, 0.015432080344060238, 0.04287259678251747, 0.021954725444172575, 0.04007608307373239, 0.03297158870882451, 0.0672068849855439, 0.04779237348506295, 0.022248693183883554, 0.04511282949177178, 0.01534257858935749, 0.017188277487138328, 0.03234822488465969])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0])\n",
      "dict_values([1, 0.55, 0.85, 1, 1, 0.9, 1, 1, 1, 1, 1, 0.25])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0])\n",
      "dict_values([1.0, 0.5, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.2, 0.2, 0.6])\n",
      "dict_values([0.603, 0.691, 0.555, 0.314, 0.381, 0.802, 0.381, 0.286, 0.531, 0.368, 0.273, 1.0])\n",
      "dict_values([0.5, 0.167, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.333, 0.0, 0.167, 0.833])\n",
      "dict_values([0.03476104365502706, 0.01821208896951591, 0.020549016364164698, 0.03640306292329207, 0.028975656587115258, 0.019656324814240924, 0.035249508687863716, 0.031811540783152924, 0.02273722412784236, 0.025149196754708166, 0.02532320280594082, 0.012701184007261642])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0])\n",
      "dict_values([1, 0.8, 0.85, 1, 1, 1, 0.65, 0.95, 0.85, 1, 0.6, 0.2, 0.95, 1, 0.65, 0.4, 0.35, 1.0, 1, 0.7, 1, 1])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.125, 0.111, 0.1, 0.091, 0.091, 0.1, 0.111, 0.125, 0.143, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0])\n",
      "dict_values([1.0, 0.625, 0.375, 0.25, 0.125, 0.375, 0.25, 0.25, 0.25, 0.375, 0.125, 0.25, 0.125, 0.25, 0.125, 0.25, 0.5, 0.125, 0.125, 0.25, 0.125, 0.125])\n",
      "dict_values([0.544, 0.917, 0.774, 0.483, 0.285, 0.604, 0.786, 0.534, 0.55, 0.576, 0.713, 0.872, 0.521, 0.526, 0.424, 0.918, 1.0, 0.671, 0.56, 0.757, 0.743, 0.523])\n",
      "dict_values([0.333, 0.167, 0.333, 0.333, 0.0, 0.167, 0.333, 0.333, 0.5, 0.333, 1.0, 0.167, 0.0, 0.333, 0.667, 0.167, 0.333, 0.167, 0.333, 0.167, 1.0, 0.167])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.03869762519368483, 0.019372123573380143, 0.024908357301371076, 0.034342307894559124, 0.04582746885504625, 0.026995157271392827, 0.01803429948273689, 0.02645127461543384, 0.02036988552283037, 0.028583838816609135, 0.01854156138619661, 0.017179322114940467, 0.024805452302804974, 0.02597609009334795, 0.025717821635298715, 0.021063781881695002, 0.015686441133199426, 0.024705751493213215, 0.038893849375130365, 0.01887609289429252, 0.029249040154113493, 0.029078580983180672])\n",
      "dict_values([0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
      "dict_values([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n",
      "dict_values([1, 0.45, 0.45, 0.3, 1, 1, 0.45, 0.35, 0.75, 1, 0.65, 0.85, 0.5, 0.35, 0.85, 0.3, 0.9])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.125, 0.111, 0.125, 0.143, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0])\n",
      "dict_values([1.0, 0.286, 0.429, 0.429, 0.286, 0.286, 0.571, 0.429, 0.143, 0.143, 0.571, 0.286, 0.857, 0.714, 0.143, 0.286, 0.143])\n",
      "dict_values([0.497, 0.691, 0.479, 1.0, 0.557, 0.757, 0.949, 0.722, 0.222, 0.186, 0.984, 0.587, 0.772, 0.799, 0.422, 0.715, 0.68])\n",
      "dict_values([0.182, 0.909, 0.091, 0.364, 0.182, 0.364, 0.273, 0.0, 0.182, 0.0, 0.364, 0.091, 1.0, 0.091, 0.273, 0.0, 0.182])\n",
      "dict_values([0.040963446034421966, 0.019328597874482215, 0.016571895253435488, 0.015322186949248957, 0.021565164356815852, 0.023122699847603685, 0.01524751773708097, 0.015372868183688895, 0.05392972807574466, 0.03542987584188831, 0.01877532589159766, 0.02168127171525839, 0.022224137962075566, 0.01538738934857083, 0.02044616573741372, 0.015070316903674575, 0.02226772564382661])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0])\n",
      "dict_values([0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.667, 0.0, 0.0, 0.0, 0.333])\n",
      "dict_values([1, 1, 0.9, 1, 1, 0.65, 0.25, 1.0, 0.7, 0.35, -0.4, 0.95, 0.65, 0.7, 0.95, 0.5, 0.9, 0.65, 0.9, 0.55, -0.15, -1.4, 0.7, 1, -0.15, 0.2, -1.1, 0.25, 0.45, 0.1, 0.9, 0.8, 0.7, 0.75, 0.3, 1, 1, 0.75, 0.6, 1, -0.8, 1.0, -0.05])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.125, 0.111, 0.1, 0.091, 0.083, 0.077, 0.071, 0.067, 0.062, 0.059, 0.056, 0.053, 0.05, 0.048, 0.045, 0.048, 0.05, 0.053, 0.056, 0.059, 0.062, 0.067, 0.071, 0.077, 0.083, 0.091, 0.1, 0.111, 0.125, 0.143, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667])\n",
      "dict_values([1.0, 0.125, 0.125, 0.125, 0.25, 0.0, 0.125, 0.25, 0.125, 0.125, 0.0, 0.25, 0.125, 0.25, 0.125, 0.25, 0.0, 0.125, 0.25, 0.125, 0.0, 0.375, 0.125, 0.0, 0.375, 0.25, 0.625, 0.25, 0.125, 0.375, 0.0, 0.0, 0.0, 0.125, 0.25, 0.0, 0.125, 0.125, 0.0, 0.0, 0.25, 0.375, 0.375])\n",
      "dict_values([0.139, 0.268, 0.299, 0.132, 0.319, 0.342, 0.37, 0.304, 0.371, 0.515, 0.58, 0.107, 0.492, 0.528, 0.192, 0.28, 0.399, 0.337, 0.329, 0.358, 0.7, 0.817, 0.402, 0.284, 0.645, 0.625, 1.0, 0.482, 0.65, 0.675, 0.369, 0.185, 0.713, 0.48, 0.454, 0.325, 0.257, 0.372, 0.675, 0.419, 0.663, 0.28, 0.592])\n",
      "dict_values([0.154, 0.077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.385, 0.0, 0.538, 0.0, 0.0, 0.154, 0.0, 0.154, 0.231, 0.462, 0.0, 0.385, 0.385, 0.231, 0.0, 0.0, 0.692, 0.077, 1.0, 0.0, 0.308, 0.0, 0.077, 0.0, 0.154, 0.154, 0.077, 0.0, 0.077, 0.0, 0.0, 0.154, 0.077, 0.077, 0.538])\n",
      "dict_values([0.05270419158442852, 0.03232658260954716, 0.034197019591403026, 0.04362527603939743, 0.03952918118902447, 0.021641194137948593, 0.01583408970914449, 0.02361375427223911, 0.024412663883220557, 0.01721659965361867, 0.015150712931877138, 0.05774896844002818, 0.022039908139178874, 0.017198546393256336, 0.023807474093657542, 0.019821435913493918, 0.02673622641702305, 0.026595580044560297, 0.02218418541215433, 0.02447727674160292, 0.01644666536557506, 0.010475135030791918, 0.018628025515419186, 0.03412896515540154, 0.01424161840272425, 0.015597654952795633, 0.011127569900213808, 0.016682261280990048, 0.019241539177268525, 0.01263615785039278, 0.03536350865299516, 0.08792631083786827, 0.019609150734632506, 0.02522991012347345, 0.01958953559621369, 0.039747827359519754, 0.029384566566423832, 0.020624865916525105, 0.026002699473588877, 0.03407225310447296, 0.01740199046426401, 0.02755880527919399, 0.017576949610258987])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "path1='C:\\\\Users\\\\Priyanka\\\\Desktop\\\\COVID_19_dataset\\\\documents\\\\'\n",
    "path2='C:\\\\Users\\\\Priyanka\\\\Desktop\\\\COVID_19_dataset\\\\summary\\\\'\n",
    "\n",
    "filelist = os.listdir(path1)\n",
    "for file in filelist:\n",
    "    f = open(path1+file, \"r\")\n",
    "    f1=open(path2+file,\"r\")\n",
    "    text=f.read()\n",
    "    text1=f1.read()\n",
    "    # documents open\n",
    "    sent_tokens = nltk.sent_tokenize(text)\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    # summary open\n",
    "    sent_tokens1 = nltk.sent_tokenize(text1)\n",
    "    word_tokens1 = nltk.word_tokenize(text1)\n",
    "    word_tokens_lower=[word.lower() for word in word_tokens]\n",
    "    stopWords = list(set(stopwords.words(\"english\")))\n",
    "    word_tokens_refined=[x for x in word_tokens_lower if x not in stopWords]\n",
    "    g=cue_phrases()\n",
    "    z=list(g.keys())\n",
    "    g=list(g.values())\n",
    "    h=numeric_data()\n",
    "    h=list(h.values())\n",
    "    i=sent_len_score()\n",
    "    i=list(i.values())\n",
    "    j=sentence_position()\n",
    "    j=list(j.values())   \n",
    "    p=upper_case()\n",
    "    p=list(p.values())\n",
    "    l=head_match()\n",
    "    l=list(l.values())\n",
    "    m=word_frequency()\n",
    "    m=list(m.values())\n",
    "    n=proper_noun()\n",
    "    n=list(n.values())\n",
    "    p=tfidf()\n",
    "    p=list(p.values())\n",
    "    label={}\n",
    "    for sentence in sent_tokens:\n",
    "        label[sentence]=0\n",
    "        for sentence1 in sent_tokens1:\n",
    "            if(sentence==sentence1):\n",
    "                label[sentence]+=1\n",
    "                \n",
    "    o=list(label.values())\n",
    "    df = df.append(pd.DataFrame({'a': g,'b': h,'c': i,'d': j,'upper': p,'f': l,'g': m,'h': n,'tfidf':p,'key': z,'label': o}), ignore_index=True)\n",
    "    \n",
    "    f.close()\n",
    "    f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>upper</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>key</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.011672</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.011672</td>\n",
       "      <td>Success from two leading coronavirus vaccine p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.005963</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.005963</td>\n",
       "      <td>The fact that two coronavirus vaccines recentl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.018060</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.018060</td>\n",
       "      <td>The studies showed both vaccines provided stro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.011463</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.011463</td>\n",
       "      <td>\"With the very good news from Pfizer and Moder...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.019715</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.019715</td>\n",
       "      <td>While Gates didn't delve into the scientific r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.021603</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.021603</td>\n",
       "      <td>All the leading vaccine candidates target the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.013236</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013236</td>\n",
       "      <td>Early-stage clinical trials showed all the sho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.013051</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013051</td>\n",
       "      <td>But the only way to know if that translates to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.017679</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017679</td>\n",
       "      <td>The scientific success has turned the top chal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.015927</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.015927</td>\n",
       "      <td>Gates noted that the world will be supply cons...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.029856</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.029856</td>\n",
       "      <td>World Health Assembly charts course for COVID-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.014726</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.014726</td>\n",
       "      <td>As health leaders prepare to gather for a virt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.030228</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030228</td>\n",
       "      <td>First, we can beat COVID-19 with science, solu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.025203</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.025203</td>\n",
       "      <td>More than 47 million COVID-19 cases have now b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.019574</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.019574</td>\n",
       "      <td>Although this is a global crisis, many countri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.014236</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014236</td>\n",
       "      <td>For the first time, the world has rallied behi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.027735</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.027735</td>\n",
       "      <td>The Access to COVID-19 Tools (ACT) Accelerator...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.032934</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.032934</td>\n",
       "      <td>Second, we must not backslide on our critical ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>The COVID-19 pandemic is a sobering reminder t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.018653</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.018653</td>\n",
       "      <td>It reminds us why WHO’s ‘triple billion’ targe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.013112</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.013112</td>\n",
       "      <td>Since May, Member States have adopted a number...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.011408</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.894</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.011408</td>\n",
       "      <td>The resumed session will discuss a 10-year-pla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.029358</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.029358</td>\n",
       "      <td>Third, we must prepare for the next pandemic now.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.013608</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.013608</td>\n",
       "      <td>We’ve seen this past year that countries with ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>0.364</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>The WHA will consider a draft resolution (EB14...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.014587</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.014587</td>\n",
       "      <td>This resolution calls on the global health com...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.022517</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.022517</td>\n",
       "      <td>\\nEnsuring Cambodia has supplies and access to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.017861</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.017861</td>\n",
       "      <td>“As long as the virus continues to circulate a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.012807</td>\n",
       "      <td>0.429</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.012807</td>\n",
       "      <td>“We must continue to detect, test, isolate and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.016669</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.016669</td>\n",
       "      <td>The COVID-19 Supply Chain System, coordinated ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.017199</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.017199</td>\n",
       "      <td>And Duke's research found that the \"vast major...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.023807</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.023807</td>\n",
       "      <td>Some middle-income countries with manufacturin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.019821</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.019821</td>\n",
       "      <td>While other countries with the infrastructure ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.026736</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.026736</td>\n",
       "      <td>India's Serum Institute, for example, has comm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.026596</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.026596</td>\n",
       "      <td>Meanwhile, Indonesia is partnering with Chines...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.022184</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022184</td>\n",
       "      <td>Because we do not yet know which vaccines will...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.024477</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.024477</td>\n",
       "      <td>India, the EU, the US, Canada and the UK are a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.016447</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.016447</td>\n",
       "      <td>The World Health Organization (WHO) told the B...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.40</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.010475</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.010475</td>\n",
       "      <td>Delivering a limited supply to the world\\nAndr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.018628</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.018628</td>\n",
       "      <td>Experts note that we do not know yet how many ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.034129</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.034129</td>\n",
       "      <td>Deals are still being made, and questions rema...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.014242</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.014242</td>\n",
       "      <td>According to Chandrakant Lahariya, co-author o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.015598</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.015598</td>\n",
       "      <td>\"There are vaccines developed in India, and wi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.011128</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.011128</td>\n",
       "      <td>Moderna Covid vaccine shows nearly 95% protect...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.016682</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.016682</td>\n",
       "      <td>\"However, the big asterisk is that if there ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.019242</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.019242</td>\n",
       "      <td>Ms Silverman said recent announcements about s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.012636</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012636</td>\n",
       "      <td>But she added: \"There is very little likelihoo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.035364</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.035364</td>\n",
       "      <td>Pfizer says it hopes to produce up to 50 milli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.087926</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.087926</td>\n",
       "      <td>Each person needs two doses.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.019609</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.019609</td>\n",
       "      <td>\"Just doing the maths… you can see it's not en...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.025230</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.025230</td>\n",
       "      <td>However, she says now Moderna has also shown s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.019590</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.019590</td>\n",
       "      <td>The Moderna vaccine also has fewer requirement...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.039748</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.039748</td>\n",
       "      <td>A new landmark distribution plan\\nOf course in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.029385</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.029385</td>\n",
       "      <td>The WHO estimates that nearly 20 million infan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.020625</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020625</td>\n",
       "      <td>Research shows that during the 2009 \"swine flu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.026003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.026003</td>\n",
       "      <td>\"We talk about the 90/10 divide in global heal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.034072</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.034072</td>\n",
       "      <td>This is part of that story,\" Ms Wenham said.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.017402</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.017402</td>\n",
       "      <td>\"But there's a difference between the fact tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.027559</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.027559</td>\n",
       "      <td>A landmark global vaccine plan known as Covax ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.017577</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.017577</td>\n",
       "      <td>The joint initiative - between the Gavi vaccin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       a      b     c      d     upper      f      g      h     tfidf  \\\n",
       "0    0.0  0.000  0.80  1.000  0.011672  0.870  0.484  0.364  0.011672   \n",
       "1    0.0  0.000 -2.25  0.500  0.005963  1.000  1.000  1.000  0.005963   \n",
       "2    0.0  0.000  1.00  0.333  0.018060  0.304  0.189  0.000  0.018060   \n",
       "3    1.0  0.000 -0.10  0.250  0.011463  0.478  0.755  0.909  0.011463   \n",
       "4    0.0  0.000  1.00  0.200  0.019715  0.130  0.238  0.091  0.019715   \n",
       "5    0.0  0.000  1.00  0.200  0.021603  0.174  0.158  0.000  0.021603   \n",
       "6    0.0  0.000  0.60  0.250  0.013236  0.130  0.305  0.000  0.013236   \n",
       "7    0.0  0.000  0.35  0.333  0.013051  0.130  0.191  0.000  0.013051   \n",
       "8    0.0  0.000  1.00  0.500  0.017679  0.261  0.212  0.000  0.017679   \n",
       "9    0.0  1.000  0.85  1.000  0.015927  0.217  0.263  0.000  0.015927   \n",
       "10   0.0  0.000  1.00  1.000  0.029856  1.000  0.497  0.500  0.029856   \n",
       "11   0.0  0.000  0.60  0.500  0.014726  0.455  0.904  0.625  0.014726   \n",
       "12   1.0  0.000  1.00  0.333  0.030228  0.182  0.479  0.000  0.030228   \n",
       "13   1.0  1.000  0.80  0.250  0.025203  0.182  0.545  0.125  0.025203   \n",
       "14   0.0  0.000  0.80  0.200  0.019574  0.182  0.471  0.000  0.019574   \n",
       "15   1.0  0.000 -0.05  0.167  0.014236  0.273  0.545  0.000  0.014236   \n",
       "16   0.0  0.000  1.00  0.143  0.027735  0.182  0.504  0.625  0.027735   \n",
       "17   1.0  0.000  1.00  0.125  0.032934  0.182  0.516  0.125  0.032934   \n",
       "18   0.0  0.000  1.00  0.125  0.020310  0.273  0.549  0.125  0.020310   \n",
       "19   0.0  0.000  0.45  0.143  0.018653  0.091  0.569  0.125  0.018653   \n",
       "20   0.0  1.000 -0.35  0.167  0.013112  0.091  0.762  0.875  0.013112   \n",
       "21   0.0  1.000 -0.80  0.200  0.011408  0.364  0.894  1.000  0.011408   \n",
       "22   1.0  0.000  1.00  0.250  0.029358  0.091  0.427  0.000  0.029358   \n",
       "23   0.0  0.000  0.40  0.333  0.013608  0.182  0.667  0.250  0.013608   \n",
       "24   0.0  1.000  0.15  0.500  0.013313  0.364  1.000  0.875  0.013313   \n",
       "25   0.0  0.000  0.50  1.000  0.014587  0.364  0.530  0.125  0.014587   \n",
       "26   0.0  0.000  1.00  1.000  0.022517  1.000  0.427  0.200  0.022517   \n",
       "27   0.0  0.000  0.60  0.500  0.017861  0.286  0.688  0.300  0.017861   \n",
       "28   0.0  0.000 -0.65  0.333  0.012807  0.429  1.000  0.400  0.012807   \n",
       "29   0.0  0.000  0.50  0.250  0.016669  0.714  0.674  0.900  0.016669   \n",
       "..   ...    ...   ...    ...       ...    ...    ...    ...       ...   \n",
       "156  0.0  0.000  0.70  0.071  0.017199  0.250  0.528  0.154  0.017199   \n",
       "157  0.0  0.000  0.95  0.067  0.023807  0.125  0.192  0.000  0.023807   \n",
       "158  0.0  0.000  0.50  0.062  0.019821  0.250  0.280  0.154  0.019821   \n",
       "159  1.0  0.000  0.90  0.059  0.026736  0.000  0.399  0.231  0.026736   \n",
       "160  0.0  0.000  0.65  0.056  0.026596  0.125  0.337  0.462  0.026596   \n",
       "161  0.0  0.000  0.90  0.053  0.022184  0.250  0.329  0.000  0.022184   \n",
       "162  1.0  0.000  0.55  0.050  0.024477  0.125  0.358  0.385  0.024477   \n",
       "163  1.0  0.000 -0.15  0.048  0.016447  0.000  0.700  0.385  0.016447   \n",
       "164  0.0  0.000 -1.40  0.045  0.010475  0.375  0.817  0.231  0.010475   \n",
       "165  0.0  0.000  0.70  0.048  0.018628  0.125  0.402  0.000  0.018628   \n",
       "166  0.0  0.000  1.00  0.050  0.034129  0.000  0.284  0.000  0.034129   \n",
       "167  1.0  0.000 -0.15  0.053  0.014242  0.375  0.645  0.692  0.014242   \n",
       "168  0.0  0.000  0.20  0.056  0.015598  0.250  0.625  0.077  0.015598   \n",
       "169  0.0  0.333 -1.10  0.059  0.011128  0.625  1.000  1.000  0.011128   \n",
       "170  0.0  0.000  0.25  0.062  0.016682  0.250  0.482  0.000  0.016682   \n",
       "171  0.0  0.333  0.45  0.067  0.019242  0.125  0.650  0.308  0.019242   \n",
       "172  0.0  0.000  0.10  0.071  0.012636  0.375  0.675  0.000  0.012636   \n",
       "173  0.0  1.000  0.90  0.077  0.035364  0.000  0.369  0.077  0.035364   \n",
       "174  0.0  0.000  0.80  0.083  0.087926  0.000  0.185  0.000  0.087926   \n",
       "175  0.0  0.000  0.70  0.091  0.019609  0.000  0.713  0.154  0.019609   \n",
       "176  1.0  0.000  0.75  0.100  0.025230  0.125  0.480  0.154  0.025230   \n",
       "177  0.0  0.000  0.30  0.111  0.019590  0.250  0.454  0.077  0.019590   \n",
       "178  0.0  0.000  1.00  0.125  0.039748  0.000  0.325  0.000  0.039748   \n",
       "179  0.0  0.333  1.00  0.143  0.029385  0.125  0.257  0.077  0.029385   \n",
       "180  0.0  0.333  0.75  0.167  0.020625  0.125  0.372  0.000  0.020625   \n",
       "181  0.0  0.667  0.60  0.200  0.026003  0.000  0.675  0.000  0.026003   \n",
       "182  0.0  0.000  1.00  0.250  0.034072  0.000  0.419  0.154  0.034072   \n",
       "183  1.0  0.000 -0.80  0.333  0.017402  0.250  0.663  0.077  0.017402   \n",
       "184  0.0  0.000  1.00  0.500  0.027559  0.375  0.280  0.077  0.027559   \n",
       "185  0.0  0.333 -0.05  1.000  0.017577  0.375  0.592  0.538  0.017577   \n",
       "\n",
       "                                                   key label  \n",
       "0    Success from two leading coronavirus vaccine p...     1  \n",
       "1    The fact that two coronavirus vaccines recentl...     1  \n",
       "2    The studies showed both vaccines provided stro...     0  \n",
       "3    \"With the very good news from Pfizer and Moder...     1  \n",
       "4    While Gates didn't delve into the scientific r...     0  \n",
       "5    All the leading vaccine candidates target the ...     0  \n",
       "6    Early-stage clinical trials showed all the sho...     0  \n",
       "7    But the only way to know if that translates to...     0  \n",
       "8    The scientific success has turned the top chal...     0  \n",
       "9    Gates noted that the world will be supply cons...     0  \n",
       "10   World Health Assembly charts course for COVID-...     1  \n",
       "11   As health leaders prepare to gather for a virt...     1  \n",
       "12   First, we can beat COVID-19 with science, solu...     1  \n",
       "13   More than 47 million COVID-19 cases have now b...     0  \n",
       "14   Although this is a global crisis, many countri...     1  \n",
       "15   For the first time, the world has rallied behi...     0  \n",
       "16   The Access to COVID-19 Tools (ACT) Accelerator...     0  \n",
       "17   Second, we must not backslide on our critical ...     1  \n",
       "18   The COVID-19 pandemic is a sobering reminder t...     0  \n",
       "19   It reminds us why WHO’s ‘triple billion’ targe...     0  \n",
       "20   Since May, Member States have adopted a number...     1  \n",
       "21   The resumed session will discuss a 10-year-pla...     0  \n",
       "22   Third, we must prepare for the next pandemic now.     1  \n",
       "23   We’ve seen this past year that countries with ...     0  \n",
       "24   The WHA will consider a draft resolution (EB14...     0  \n",
       "25   This resolution calls on the global health com...     0  \n",
       "26   \\nEnsuring Cambodia has supplies and access to...     0  \n",
       "27   “As long as the virus continues to circulate a...     1  \n",
       "28   “We must continue to detect, test, isolate and...     1  \n",
       "29   The COVID-19 Supply Chain System, coordinated ...     0  \n",
       "..                                                 ...   ...  \n",
       "156  And Duke's research found that the \"vast major...     0  \n",
       "157  Some middle-income countries with manufacturin...     0  \n",
       "158  While other countries with the infrastructure ...     0  \n",
       "159  India's Serum Institute, for example, has comm...     1  \n",
       "160  Meanwhile, Indonesia is partnering with Chines...     0  \n",
       "161  Because we do not yet know which vaccines will...     0  \n",
       "162  India, the EU, the US, Canada and the UK are a...     1  \n",
       "163  The World Health Organization (WHO) told the B...     0  \n",
       "164  Delivering a limited supply to the world\\nAndr...     0  \n",
       "165  Experts note that we do not know yet how many ...     0  \n",
       "166  Deals are still being made, and questions rema...     0  \n",
       "167  According to Chandrakant Lahariya, co-author o...     0  \n",
       "168  \"There are vaccines developed in India, and wi...     0  \n",
       "169  Moderna Covid vaccine shows nearly 95% protect...     1  \n",
       "170  \"However, the big asterisk is that if there ar...     0  \n",
       "171  Ms Silverman said recent announcements about s...     0  \n",
       "172  But she added: \"There is very little likelihoo...     0  \n",
       "173  Pfizer says it hopes to produce up to 50 milli...     1  \n",
       "174                       Each person needs two doses.     0  \n",
       "175  \"Just doing the maths… you can see it's not en...     0  \n",
       "176  However, she says now Moderna has also shown s...     1  \n",
       "177  The Moderna vaccine also has fewer requirement...     0  \n",
       "178  A new landmark distribution plan\\nOf course in...     0  \n",
       "179  The WHO estimates that nearly 20 million infan...     1  \n",
       "180  Research shows that during the 2009 \"swine flu...     0  \n",
       "181  \"We talk about the 90/10 divide in global heal...     0  \n",
       "182       This is part of that story,\" Ms Wenham said.     0  \n",
       "183  \"But there's a difference between the fact tha...     0  \n",
       "184  A landmark global vaccine plan known as Covax ...     1  \n",
       "185  The joint initiative - between the Gavi vaccin...     1  \n",
       "\n",
       "[186 rows x 11 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.to_csv('C:\\\\Users\\\\Priyanka\\\\Desktop\\\\COVID_19_dataset\\\\output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('C:\\\\Users\\\\Priyanka\\\\Desktop\\\\COVID_19_dataset\\\\output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training=df[df.columns[0:8]]\n",
    "test=df[df.columns[-1]]\n",
    "# print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "X_train, X_test, y_train, y_test = train_test_split(training, test, test_size=0.3,random_state=109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic():   \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf2 = LogisticRegression(random_state=0)\n",
    "    #Train the model using the training sets\n",
    "    clf2.fit(X_train, y_train)\n",
    "\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf2.predict(X_test) \n",
    "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.85714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Priyanka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Priyanka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# voting ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "model1 = LogisticRegression(random_state=1)\n",
    "model2 = DecisionTreeClassifier(random_state=2,max_depth=5)\n",
    "model3 = KNeighborsClassifier()\n",
    "# model4 = SVC(random_state=3,kernel='linear')\n",
    "model = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('knn',model3)], voting='hard')\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking ensemble\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "def Stacking(model,train,y,test,n_fold):\n",
    "   folds=StratifiedKFold(n_splits=n_fold,random_state=1)\n",
    "   test_pred=[0]*test.shape[0]\n",
    "   train_pred=np.empty((0,1),float)\n",
    "   for train_indices,val_indices in folds.split(train,y.values):\n",
    "      res=[]\n",
    "      x_train,x_val=train.iloc[train_indices],train.iloc[val_indices]\n",
    "      y_train,y_val=y.iloc[train_indices],y.iloc[val_indices]\n",
    "\n",
    "      model.fit(X=x_train,y=y_train)\n",
    "      train_pred=np.append(train_pred,model.predict(x_val))\n",
    "      pred=model.predict(test)\n",
    "      for i in range(len(pred)):\n",
    "            res.append(pred[i]+test_pred[i])\n",
    "      test_pred=res\n",
    "   test_pred=[each/10 for each in test_pred]\n",
    "   return test_pred,train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Priyanka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6964285714285714"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = DecisionTreeClassifier(random_state=1, max_depth=4)\n",
    "test_pred1 ,train_pred1=Stacking(model=model1,n_fold=10, train=X_train,test=X_test,y=y_train)\n",
    "train_pred1=pd.DataFrame(train_pred1)\n",
    "test_pred1=pd.DataFrame(test_pred1)\n",
    "model2 = SVC(random_state=3,kernel='linear')\n",
    "test_pred2 ,train_pred2=Stacking(model=model2,n_fold=10,train=X_train,test=X_test,y=y_train)\n",
    "train_pred2=pd.DataFrame(train_pred2)\n",
    "test_pred2=pd.DataFrame(test_pred2)\n",
    "\n",
    "df = pd.concat([train_pred1, train_pred2], axis=1)\n",
    "df_test = pd.concat([test_pred1, test_pred2], axis=1)\n",
    "# print(df_test)\n",
    "model = LogisticRegression(random_state=1)\n",
    "model.fit(df,y_train)\n",
    "# print(X_test.shape)\n",
    "model.score(df_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
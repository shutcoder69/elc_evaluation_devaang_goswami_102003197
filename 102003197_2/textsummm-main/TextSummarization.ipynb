{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextSummarization.ipynb",
      "provenance": [],
      "mount_file_id": "1ySYgE0EjxBz-ZIpKakkwmvuJw1QXqDac",
      "authorship_tag": "ABX9TyOFa14FdAXfxeJ4746hwiQr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gpriya32/text-summarization/blob/main/TextSummarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU143Jw5zpaH",
        "outputId": "0694224b-d9f3-48ae-f38c-f4fe673c32a4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import *"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fncQ4nJn0bpc",
        "outputId": "9eb70ffe-3226-477f-fc69-6a750a28179e"
      },
      "source": [
        "f=open(\"/content/sample_data/text.txt\",'r')\n",
        "text=f.read()\n",
        "print(text)\n",
        "f.close()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Those Who Are Resilient Stay In The Game Longer\n",
            "\n",
            "“On the mountains of truth you can never climb in vain: either you will reach a point higher up today, or you will be training your powers so that you will be able to climb higher tomorrow.” — Friedrich Nietzsche\n",
            "Challenges and setbacks are not meant to defeat you, but promote you. However, I realise after many years of defeats, it can crush your spirit and it is easier to give up than risk further setbacks and disappointments. Have you experienced this before? To be honest, I don’t have the answers. I can’t tell you what the right course of action is; only you will know. However, it’s important not to be discouraged by failure when pursuing a goal or a dream, since failure itself means different things to different people. To a person with a Fixed Mindset failure is a blow to their self-esteem, yet to a person with a Growth Mindset, it’s an opportunity to improve and find new ways to overcome their obstacles. Same failure, yet different responses. Who is right and who is wrong? Neither. Each person has a different mindset that decides their outcome. Those who are resilient stay in the game longer and draw on their inner means to succeed.I’ve coached many clients who gave up after many years toiling away at their respective goal or dream. It was at that point their biggest breakthrough came. Perhaps all those years of perseverance finally paid off. It was the 19th Century’s minister Henry Ward Beecher who once said: “One’s best success comes after their greatest disappointments.” No one knows what the future holds, so your only guide is whether you can endure repeated defeats and disappointments and still pursue your dream. Consider the advice from the American academic and psychologist Angela Duckworth who writes in Grit: The Power of Passion and Perseverance: “Many of us, it seems, quit what we start far too early and far too often. Even more than the effort a gritty person puts in on a single day, what matters is that they wake up the next day, and the next, ready to get on that treadmill and keep going.”I know one thing for certain: don’t settle for less than what you’re capable of, but strive for something bigger. Some of you reading this might identify with this message because it resonates with you on a deeper level. For others, at the end of their tether the message might be nothing more than a trivial pep talk. What I wish to convey irrespective of where you are in your journey is: NEVER settle for less. If you settle for less, you will receive less than you deserve and convince yourself you are justified to receive it.“Two people on a precipice over Yosemite Valley” by Nathan Shipps on Unsplash\n",
            "Develop A Powerful Vision Of What You Want\n",
            "“Your problem is to bridge the gap which exists between where you are now and the goal you intend to reach.” — Earl Nightingale\n",
            "I recall a passage my father often used growing up in 1990s: “Don’t tell me your problems unless you’ve spent weeks trying to solve them yourself.” That advice has echoed in my mind for decades and became my motivator. Don’t leave it to other people or outside circumstances to motivate you because you will be let down every time. It must come from within you. Gnaw away at your problems until you solve them or find a solution. Problems are not stop signs, they are advising you that more work is required to overcome them. Most times, problems help you gain a skill or develop the resources to succeed later. So embrace your challenges and develop the grit to push past them instead of retreat in resignation. Where are you settling in your life right now? Could you be you playing for bigger stakes than you are? Are you willing to play bigger even if it means repeated failures and setbacks? You should ask yourself these questions to decide whether you’re willing to put yourself on the line or settle for less. And that’s fine if you’re content to receive less, as long as you’re not regretful later.If you have not achieved the success you deserve and are considering giving up, will you regret it in a few years or decades from now? Only you can answer that, but you should carve out time to discover your motivation for pursuing your goals. It’s a fact, if you don’t know what you want you’ll get what life hands you and it may not be in your best interest, affirms author Larry Weidel: “Winners know that if you don’t figure out what you want, you’ll get whatever life hands you.” The key is to develop a powerful vision of what you want and hold that image in your mind. Nurture it daily and give it life by taking purposeful action towards it.Vision + desire + dedication + patience + daily action leads to astonishing success. Are you willing to commit to this way of life or jump ship at the first sign of failure? I’m amused when I read questions written by millennials on Quora who ask how they can become rich and famous or the next Elon Musk. Success is a fickle and long game with highs and lows. Similarly, there are no assurances even if you’re an overnight sensation, to sustain it for long, particularly if you don’t have the mental and emotional means to endure it. This means you must rely on the one true constant in your favour: your personal development. The more you grow, the more you gain in terms of financial resources, status, success — simple. If you leave it to outside conditions to dictate your circumstances, you are rolling the dice on your future.So become intentional on what you want out of life. Commit to it. Nurture your dreams. Focus on your development and if you want to give up, know what’s involved before you take the plunge. Because I assure you, someone out there right now is working harder than you, reading more books, sleeping less and sacrificing all they have to realise their dreams and it may contest with yours. Don’t leave your dreams to chance.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laLeL4Fwct5K"
      },
      "source": [
        "def text_cleaner(text):\n",
        "    rules = [\n",
        "        {r'>\\s+': u'>'},  # remove spaces after a tag opens or closes\n",
        "        {r'\\s+': u' '},  # replace consecutive spaces\n",
        "        {r'\\s*<br\\s*/?>\\s*': u'\\n'},  # newline after a <br>\n",
        "        {r'</(div)\\s*>\\s*': u'\\n'},  # newline after </p> and </div> and <h1/>...\n",
        "        {r'</(p|h\\d)\\s*>\\s*': u'\\n\\n'},  # newline after </p> and </div> and <h1/>...\n",
        "        {r'<head>.*<\\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>\n",
        "        {r'<a\\s+href=\"([^\"]+)\"[^>]*>.*</a>': r'\\1'},  # show links instead of texts\n",
        "        {r'[ \\t]*<[^<]*?/?>': u''},  # remove remaining tags\n",
        "        {r'^\\s+': u''}  # remove spaces at the beginning\n",
        "    ]\n",
        "    for rule in rules:\n",
        "      for (k, v) in rule.items():\n",
        "          regex = re.compile(k)\n",
        "          text = regex.sub(v, text)\n",
        "      text = text.rstrip()\n",
        "    return text\n",
        "    "
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXspirdHst_E"
      },
      "source": [
        "ps=PorterStemmer()\n",
        "def steming(words):\n",
        "  stem=[]\n",
        "  # play,played..\n",
        "  for word in words:\n",
        "    stem.append(ps.stem(word))\n",
        "  # print(stem)\n",
        "  return stem"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpj2S8mn1vE_"
      },
      "source": [
        "# preprocessing \n",
        "text=text_cleaner(text)\n",
        "# print(text)\n",
        "sent_tokens=sent_tokenize(text)\n",
        "word_tokens=word_tokenize(text)\n",
        "word_tokens=[word.lower() for word in word_tokens]\n",
        "\n",
        "# removing stopwords\n",
        "stopwords=list(set(stopwords.words('english')))\n",
        "word_token_ref=[word for word in word_tokens if word not in stopwords]\n",
        "\n",
        "word_final=steming(word_token_ref)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbL9acLg2zp_",
        "outputId": "6bc61896-cf6a-4a14-84f9-ba8f7f97e084"
      },
      "source": [
        "freq={}\n",
        "for word in word_final:\n",
        "  if word in freq:\n",
        "    freq[word]+=1\n",
        "  else:\n",
        "    freq[word]=1\n",
        "print(len(word_tokens))\n",
        "print(len(freq))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1214\n",
            "331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtjcOgVkxr0Y"
      },
      "source": [
        "def stemSentence(sentence):\n",
        "    sentence=sentence.lower()\n",
        "    token_words=word_tokenize(sentence)\n",
        "    stem_sentence=[]\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(ps.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return stem_sentence"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps225rMU3PIR"
      },
      "source": [
        "# computing the score of each sentence\n",
        "\n",
        "def sentence_score():\n",
        "      sentval={}\n",
        "      sumval=0\n",
        "      for sentence in sent_tokens:\n",
        "        sentval[sentence]=0\n",
        "        for word, count in freq.items():\n",
        "          sent=stemSentence(sentence)\n",
        "          if word in sent:\n",
        "            sentval[sentence]+=count\n",
        "      maxi=max(sentval.values())\n",
        "      for key,val in sentval.items():\n",
        "        sentval[key]=val/maxi\n",
        "      print(sentval.values())\n",
        "      return sentval\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eEYuMpE7qap"
      },
      "source": [
        "# from nltk.stem import WordNetLemmatizer\n",
        "# nltk.download('wordnet')\n",
        "# lemmatizer = WordNetLemmatizer()\n",
        "# print(lemmatizer.lemmatize(\"cats\"))"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqpXOes78Y-L"
      },
      "source": [
        "# calculating cue -phrase\n",
        "\n",
        "def cue_phrase_cal():\n",
        "    cue_phrase=['firstly','secondly','thirdly','assumption','anyway','conclusion','nutshell','moreover']\n",
        "    qphrase={}\n",
        "    for sent in sent_tokens:\n",
        "      qphrase[sent]=0\n",
        "      tokens_word=word_tokenize(sent)\n",
        "      for word in word_tokens:\n",
        "        if word.lower() in cue_phrase:\n",
        "          qphrase[sent]+=1\n",
        "\n",
        "    maxi=max(qphrase.values())\n",
        "\n",
        "    for key,val in qphrase.items():\n",
        "      try:\n",
        "        qphrase[key]=val/maxi\n",
        "      except: \n",
        "        pass\n",
        "    print(qphrase.values())\n",
        "    return qphrase"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyh_EBKGnoT-"
      },
      "source": [
        "# calculating upper case \n",
        "\n",
        "def upper_cal():\n",
        "    upper={}\n",
        "    for sent in sent_tokens:\n",
        "      upper[sent]=0\n",
        "      tokens_word=word_tokenize(sent)\n",
        "      for word in word_tokens:\n",
        "        if word.isupper():\n",
        "          upper[sent]+=1\n",
        "\n",
        "    maxi=max(upper.values())\n",
        "\n",
        "    for key,val in upper.items():\n",
        "      try:\n",
        "        upper[key]=val/maxi\n",
        "      except: \n",
        "        pass\n",
        "    print(upper.values())\n",
        "    return upper"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugtGaRmxrBEw"
      },
      "source": [
        "# calculating upper case \n",
        "\n",
        "def digit_cal():\n",
        "    digit={}\n",
        "    for sent in sent_tokens:\n",
        "      digit[sent]=0\n",
        "      tokens_word=word_tokenize(sent)\n",
        "      for word in word_tokens:\n",
        "        if word.isdigit():\n",
        "          digit[sent]+=1\n",
        "\n",
        "    maxi=max(digit.values())\n",
        "\n",
        "    for key,val in digit.items():\n",
        "      try:\n",
        "        digit[key]=val/maxi\n",
        "      except: \n",
        "        pass\n",
        "    print(digit.values())\n",
        "    return digit"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hXbTo_HrnFy"
      },
      "source": [
        "# calculating proper noun \n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "def pnoun_cal():\n",
        "    pnoun={}\n",
        "    for sent in sent_tokens:\n",
        "      pnoun[sent]=0\n",
        "      tagged_sent=pos_tag(sent.split())\n",
        "      proper=[word for word,tag in tagged_sent if tag=='NNP']\n",
        "      pnoun[sent]=len(proper)\n",
        "\n",
        "    maxi=max(pnoun.values())\n",
        "\n",
        "    for key,val in pnoun.items():\n",
        "      try:\n",
        "        pnoun[key]=val/maxi\n",
        "      except: \n",
        "        pass\n",
        "    print(pnoun.values())\n",
        "    return pnoun"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGGnzcjC9g_g"
      },
      "source": [
        "def sentence_len_cal():\n",
        "    sent_len={}\n",
        "    for sent in sent_tokens:\n",
        "      sent_len[sent]=0\n",
        "      word_tokens=word_tokenize(sent)\n",
        "      length=len(word_tokens)\n",
        "      if length in range(0,10):\n",
        "        sent_len[sent]=1-0.05*(10-length)\n",
        "      elif length in range(10,20):\n",
        "        sent_len[sent]=1\n",
        "      else:\n",
        "        sent_len[sent]=1-0.05*(length-20)\n",
        "    print(sent_len.values())\n",
        "    return sent_len"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCAQu8mc_TdS"
      },
      "source": [
        "def sentence_pos():\n",
        "  sentence_position={}\n",
        "  n=1\n",
        "  N=len(sent_tokens)\n",
        "  for sent in sent_tokens:\n",
        "    a=1/n\n",
        "    b=1/(N+1-n)\n",
        "    sentence_position[sent]=max(a,b)\n",
        "    n=n+1\n",
        "  print(sentence_position.values())\n",
        "  return sentence_position"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09mx0mZnuK5M"
      },
      "source": [
        "# calculating upper case \n",
        "\n",
        "def heading_cal():\n",
        "    head={}\n",
        "    head_tokens=stemSentence(sent_tokens[0])\n",
        "\n",
        "    for sent in sent_tokens:\n",
        "      head[sent]=0\n",
        "      word_tokens=stemSentence(sent)\n",
        "\n",
        "      for word in word_tokens:\n",
        "        if word not in stopwords and word in head_tokens:\n",
        "          head[sent]+=1\n",
        "\n",
        "    maxi=max(head.values())\n",
        "\n",
        "    for key,val in head.items():\n",
        "      try:\n",
        "        head[key]=val/maxi\n",
        "      except: \n",
        "        pass\n",
        "    print(head.values())\n",
        "    return head"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgNENxrcYPT-"
      },
      "source": [
        "import math\n",
        "\n",
        "def create_frequency_matrix(sentences):\n",
        "    frequency_matrix = {}\n",
        "\n",
        "    for sent in sentences:\n",
        "        freq_table = {}\n",
        "        words = word_tokenize(sent)\n",
        "        for word in words:\n",
        "            word = word.lower()\n",
        "            word = ps.stem(word)\n",
        "            if word in stopwords:\n",
        "                pass\n",
        "            if word in freq_table:\n",
        "                freq_table[word] += 1\n",
        "            else:\n",
        "                freq_table[word] = 1\n",
        "\n",
        "        frequency_matrix[sent] = freq_table\n",
        "\n",
        "    return frequency_matrix\n",
        "\n",
        "\n",
        "def create_tf_matrix(freq_matrix):\n",
        "    tf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        tf_table = {}\n",
        "\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, count in f_table.items():\n",
        "            tf_table[word] = count / count_words_in_sentence\n",
        "\n",
        "        tf_matrix[sent] = tf_table\n",
        "\n",
        "    return tf_matrix\n",
        "\n",
        "\n",
        "def create_documents_per_words(freq_matrix):\n",
        "    word_per_doc_table = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        for word, count in f_table.items():\n",
        "          # occureance of word in sentences\n",
        "            if word in word_per_doc_table:\n",
        "                word_per_doc_table[word] += 1\n",
        "            else:\n",
        "                word_per_doc_table[word] = 1\n",
        "\n",
        "    return word_per_doc_table\n",
        "\n",
        "\n",
        "def create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
        "  # total_documents=total_sentences\n",
        "  # inverse document frequency\n",
        "    idf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        idf_table = {}\n",
        "\n",
        "        for word in f_table.keys():\n",
        "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
        "\n",
        "        idf_matrix[sent] = idf_table\n",
        "\n",
        "    return idf_matrix\n",
        "\n",
        "\n",
        "def create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
        "    tf_idf_matrix = {}\n",
        "\n",
        "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
        "\n",
        "        tf_idf_table = {}\n",
        "\n",
        "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
        "                                                    f_table2.items()):  \n",
        "            tf_idf_table[word1] = float(value1 * value2)\n",
        "\n",
        "        tf_idf_matrix[sent1] = tf_idf_table\n",
        "\n",
        "    return tf_idf_matrix\n",
        "\n",
        "\n",
        "def score_sent(tf_idf_matrix):\n",
        "\n",
        "    sentenceValue = {}\n",
        "\n",
        "    for sent, f_table in tf_idf_matrix.items():\n",
        "        total_score_per_sentence = 0\n",
        "\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, score in f_table.items():\n",
        "            total_score_per_sentence += 0.5*score\n",
        "\n",
        "        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n",
        "\n",
        "    return sentenceValue\n",
        "\n",
        "\n",
        "def tfidf():\n",
        "    sentences = sent_tokens\n",
        "    total_documents = len(sentences)\n",
        "\n",
        "    freq_matrix = create_frequency_matrix(sentences)\n",
        "    #print(freq_matrix)\n",
        "\n",
        "    tf_matrix = create_tf_matrix(freq_matrix)\n",
        "    #print(tf_matrix)\n",
        "\n",
        "    count_doc_per_words = create_documents_per_words(freq_matrix)\n",
        "    #print(count_doc_per_words)\n",
        "\n",
        "    idf_matrix = create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
        "    \n",
        "    tf_idf_matrix = create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
        "\n",
        "    tfidf_score = score_sent(tf_idf_matrix)\n",
        "\n",
        "    print(tfidf_score.values())\n",
        "    return tfidf_score\n",
        "\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dmV0f65RzNG",
        "outputId": "752b9428-9a70-4027-dd8a-9bf1bce14601"
      },
      "source": [
        "scores_fun=[sentence_score(),cue_phrase_cal(),upper_cal(),digit_cal(),sentence_pos(),sentence_len_cal(),pnoun_cal(),tfidf(),heading_cal()]\n",
        "total_score={}\n",
        "for fun in scores_fun:\n",
        "  arr=fun\n",
        "  for sent in sent_tokens:\n",
        "    total_score[sent]=0\n",
        "    if sent in arr:\n",
        "      total_score[sent]+=arr[sent]\n",
        "\n",
        "avg=np.mean(list(total_score.values()))\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_values([0.6592920353982301, 0.4690265486725664, 0.035398230088495575, 0.47345132743362833, 0.3805309734513274, 0.6283185840707964, 0.5929203539823009, 0.4026548672566372, 0.05309734513274336, 0.18141592920353983, 0.24336283185840707, 0.4778761061946903, 0.19911504424778761, 0.2168141592920354, 0.7964601769911505, 0.5575221238938053, 0.7876106194690266, 0.22566371681415928, 0.3938053097345133, 0.29646017699115046, 1.0, 0.3893805309734513, 0.19911504424778761, 0.2345132743362832, 0.40707964601769914, 0.4424778761061947, 0.24778761061946902, 0.09734513274336283, 0.061946902654867256, 0.1504424778761062, 0.40707964601769914, 0.49557522123893805, 0.42035398230088494, 0.8053097345132744, 0.336283185840708, 0.13716814159292035, 0.3805309734513274, 0.23893805309734514, 0.5575221238938053, 0.33185840707964603, 0.4247787610619469, 0.4646017699115044, 0.18584070796460178, 0.21238938053097345, 0.5796460176991151, 0.49557522123893805, 0.336283185840708])\n",
            "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "dict_values([1.0, 0.5, 0.3333333333333333, 0.25, 0.2, 0.16666666666666666, 0.14285714285714285, 0.125, 0.1111111111111111, 0.1, 0.09090909090909091, 0.08333333333333333, 0.07692307692307693, 0.07142857142857142, 0.06666666666666667, 0.0625, 0.058823529411764705, 0.05555555555555555, 0.05263157894736842, 0.05, 0.047619047619047616, 0.045454545454545456, 0.043478260869565216, 0.041666666666666664, 0.043478260869565216, 0.045454545454545456, 0.047619047619047616, 0.05, 0.05263157894736842, 0.05555555555555555, 0.058823529411764705, 0.0625, 0.06666666666666667, 0.07142857142857142, 0.07692307692307693, 0.08333333333333333, 0.09090909090909091, 0.1, 0.1111111111111111, 0.125, 0.14285714285714285, 0.16666666666666666, 0.2, 0.25, 0.3333333333333333, 0.5, 1.0])\n",
            "dict_values([-1.35, 0.55, 0.8, 1, 1, 0.5, 0.0, 0.85, 0.9, 0.6, 1, 0.1499999999999999, 1, 1, -0.7000000000000002, -0.15000000000000013, -1.35, 1.0, 0.95, 1.0, -3.9000000000000004, 0.85, 0.85, 1, 1, 1, 1, 1, 1, 1, 0.8, -0.5, 0.95, -2.1, 0.6499999999999999, 1.0, 0.6, 1, 0.1499999999999999, 1, 0.95, 0.5, 0.7, 0.7, 0.85, 0.09999999999999998, 0.95])\n",
            "dict_values([1.3333333333333333, 0.16666666666666666, 0.0, 0.0, 0.0, 0.16666666666666666, 0.6666666666666666, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.1666666666666667, 0.0, 0.0, 0.0, 0.16666666666666666, 2.0, 0.16666666666666666, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.16666666666666666, 0.6666666666666666, 0.16666666666666666, 0.16666666666666666, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.16666666666666666])\n",
            "dict_values([0.01222793250957792, 0.01999362640728383, 0.0854731298904597, 0.030219466422894675, 0.024230841105137558, 0.02256884744482725, 0.025732209337792918, 0.07452331086498447, 0.09633619775455769, 0.21776696556793407, 0.04915479880646671, 0.017095401349240903, 0.05394049560766687, 0.06130793045798971, 0.012624001701207357, 0.02071365991375972, 0.012403437507247374, 0.035453535709772464, 0.026978544020862184, 0.021821275124854548, 0.008087187409250749, 0.023745890360881032, 0.06491460288405279, 0.03635638506802665, 0.027315195502093947, 0.028995660989473218, 0.028643472580769942, 0.040014265271925656, 0.052711565944581394, 0.028049104794106182, 0.02496108153303451, 0.015748043401954324, 0.025867818413560145, 0.014607923883813278, 0.03921063198103439, 0.027896320403374468, 0.022539583993198067, 0.047343842969162074, 0.018475594506801977, 0.02922528495936228, 0.03575347121779603, 0.020652636799125247, 0.06687122423130733, 0.08759469411597993, 0.019154097769232638, 0.01573128817943732, 0.04086510422655135])\n",
            "dict_values([0.7829457364341085, 0.26356589147286824, 0.046511627906976744, 0.10852713178294573, 0.15503875968992248, 0.2558139534883721, 0.3333333333333333, 0.06976744186046512, 0.06201550387596899, 0.023255813953488372, 0.09302325581395349, 0.32558139534883723, 0.09302325581395349, 0.08527131782945736, 0.46511627906976744, 0.3875968992248062, 0.5736434108527132, 0.16279069767441862, 0.17829457364341086, 0.17829457364341086, 1.0, 0.18604651162790697, 0.06201550387596899, 0.11627906976744186, 0.16279069767441862, 0.14728682170542637, 0.15503875968992248, 0.07751937984496124, 0.09302325581395349, 0.12403100775193798, 0.1937984496124031, 0.40310077519379844, 0.17829457364341086, 0.6976744186046512, 0.21705426356589147, 0.15503875968992248, 0.2248062015503876, 0.10852713178294573, 0.3178294573643411, 0.15503875968992248, 0.20155038759689922, 0.24806201550387597, 0.03875968992248062, 0.03875968992248062, 0.1937984496124031, 0.32558139534883723, 0.07751937984496124])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyq1tPBWMR3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a708c7f8-4413-4486-91c5-89357d500b0f"
      },
      "source": [
        "summary=\"\"\n",
        "for sentence in sent_tokens:\n",
        "  if total_score[sentence]>1.5*avg:\n",
        "    summary+=sentence+\" \"\n",
        "print(summary)\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Those Who Are Resilient Stay In The Game Longer “On the mountains of truth you can never climb in vain: either you will reach a point higher up today, or you will be training your powers so that you will be able to climb higher tomorrow.” — Friedrich Nietzsche Challenges and setbacks are not meant to defeat you, but promote you. It was the 19th Century’s minister Henry Ward Beecher who once said: “One’s best success comes after their greatest disappointments.” No one knows what the future holds, so your only guide is whether you can endure repeated defeats and disappointments and still pursue your dream. Consider the advice from the American academic and psychologist Angela Duckworth who writes in Grit: The Power of Passion and Perseverance: “Many of us, it seems, quit what we start far too early and far too often. Even more than the effort a gritty person puts in on a single day, what matters is that they wake up the next day, and the next, ready to get on that treadmill and keep going.”I know one thing for certain: don’t settle for less than what you’re capable of, but strive for something bigger. If you settle for less, you will receive less than you deserve and convince yourself you are justified to receive it.“Two people on a precipice over Yosemite Valley” by Nathan Shipps on Unsplash Develop A Powerful Vision Of What You Want “Your problem is to bridge the gap which exists between where you are now and the goal you intend to reach.” — Earl Nightingale I recall a passage my father often used growing up in 1990s: “Don’t tell me your problems unless you’ve spent weeks trying to solve them yourself.” That advice has echoed in my mind for decades and became my motivator. And that’s fine if you’re content to receive less, as long as you’re not regretful later.If you have not achieved the success you deserve and are considering giving up, will you regret it in a few years or decades from now? It’s a fact, if you don’t know what you want you’ll get what life hands you and it may not be in your best interest, affirms author Larry Weidel: “Winners know that if you don’t figure out what you want, you’ll get whatever life hands you.” The key is to develop a powerful vision of what you want and hold that image in your mind. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCpN2HVqe0HT"
      },
      "source": [
        ""
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzhrYHLdzMdS"
      },
      "source": [
        ""
      ],
      "execution_count": 62,
      "outputs": []
    }
  ]
}